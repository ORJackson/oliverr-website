---
date: '2021-11-25'
thumbnail: /assets/wearable-technology.jpg
title: Wearable Technology and the Bystander Privacy Concerns Arising from Outward-facing Cameras
description: An investigation into the privacy concerns of individuals in the vicinity of wearable devices that feature cameras

---


<h2> Abstract </h2>

Wearable devices, such as smart glasses, featuring video and sound recording capability are becoming increasingly popular. These devices pose a heightened risk to the privacy of bystanders due to their inescapable nature, unlike traditional cameras, they do not get put away. With the development of facial recognition software used by police and government, an image is enough to identify a person; in some cases, this software uses images sourced from social media (where content captured by wearable devices is often distributed). These devices threaten a bystander’s anonymity, jeopardize their right to informational self-determination, can disclose their speech, and can disclose their location, all without consent and perhaps even without their knowledge. This paper provides a brief examination of the academic background of privacy and a literature review of relevant research. It proposes a solution to the problem of bystander privacy, developing the work of Ahmad et al and Perez et al, through wearable technology with features that unambiguously demonstrate the recording state of the device (camera shutters, a clearly visible LED, and a clearly visible mute/recording button), and that feature the FacePET system which can protect the wearer from facial recognition (thus protecting both the user’s and bystander’s privacy). In order to evaluate the impact of this solution on conceptions of privacy, a study is proposed featuring a field experiment and structured interviews.  


<h2> Introduction </h2>

Imagine a scenario: you are out for an early dinner with your partner and your young child. It is summer and you opt to sit outside. You can’t help but discuss the stresses of your new job, and you talk openly with your partner. You discuss the primary school that your child attends. It is not until later that you notice the individual on the table next to you, who is sitting facing you, is wearing a pair of Ray-Ban Stories, with which a user can take photos or short videos and post them to Facebook immediately. You then notice the small LED flashing and realise that the individual may have been recording videos throughout your meal. You don’t know whether they have been deliberately recording you, but you can’t help but worry about the things you have said, the work gossip spilt, the name of the organization you work for, the name of the school your child attends, as well as all the footage of you, your partner, and your child, all posted online without your consent. If you found yourself in this scenario, would you feel your privacy had been compromised?
When Google first released the Google Glass in 2013 it was available only to “Glass Explorers”, a select few software engineers in the US who paid $1500 to be able to be the first users of the device. Google Glass was a truly innovative device, offering the means of viewing content hands-free, making voice and video calls, and taking photos and recording video, along with a host of other features. Glass however was a failure, for a host of reasons ranging from its high price tag, various health and safety concerns, and concerns for the privacy of bystanders. The public was very unaccustomed to seeing people wearing glasses featuring cameras, and at the time there was a public backlash. One piece of research in Britain and the US questioned 4000 participants on how they felt about Google Glass and found that 51% of individuals had serious concerns about their own privacy, and 61% felt there needed to be increased regulation [Woollaston 2013]. The backlash in certain parts of the US was so bad that Google Glass was banned in several nightclubs and casinos [Streitfeld 2013], and some Glass Explorers reported being confronted by people who thought they were being recorded [Garfinkel, 2014]. Google Glass was not a commercial success.
Fast forward to today and the market has changed considerably, smartphones and other devices such as smart glasses and other wearable technology capable of recording video have become far more accepted. Today multiple smart glasses featuring cameras exist, including the Vuzix blade, Xiaomi smart glasses, the Snap spectacles 3 from Snapchat, and the collaboration between Facebook (now meta) and Ray-Ban, the Ray-Ban Stories. These glasses can record video and take photos and upload them instantly to social media. They are much more affordable than when Google Glass first hit the market, for example, Ray-Ban Stories are available in the UK for less than £300. One might suppose that with the pervasiveness of smartphones, people are far more accustomed today to being recorded in public, however, the privacy concern of wearable devices featuring cameras has by no means vanished. Many are concerned that these devices present a variety of privacy concerns not presented by traditional cameras. For example, in the case of Ray-Ban Stories, there is concern over the potential for their misuse; Ireland’s Data Protection Commission, Facebook’s lead privacy regulator in Europe, warned Facebook that the LED used to demonstrate that the glasses are recording video is too small. The DPC asked Facebook to demonstrate that the LED on the glasses, which appear very similar to a normal pair of Ray-Ban sunglasses, is an effective means of alerting bystanders to the fact that the device is recording [Lomas 2022].
In this paper, I will provide a literature review of the academic basis for privacy, the research on the problem of bystander privacy concerns, and some solutions that have been proposed. I will then propose my own solution that develops the work of Ahmad et al. and Perez et al. and propose a study to evaluate this solution.

<h2>Literature Review</h2>

<h3> Privacy</h3>

Iachello and Hong carried out thorough academic research into the topic of privacy in HCI, noting that government reports and media coverage testify to the public’s concern over privacy when it comes to emerging technologies, and equally that application developers are wary that privacy concerns can present an obstacle to the adoption of their technology [Iachello and Hong 2007]. Privacy is a very important issue in HCI both for users and developers, however, the literature on privacy is spread across many disciplines ranging from law to philosophy to computer science. These researchers explain the many conceptions of privacy that people have by referencing A F Westin who describes four states of privacy: “solitude, intimacy, anonymity, and reserve” [Iachello and Hong 2007], and quote R S Murphy who provides several expressions of privacy including “The right to prevent commercial publicity of one’s own name and image” and “the control of information concerning an individual’s person” [Iachello and Hong 2017]. Anonymity is a central pillar of privacy, but with the rise of facial recognition technology, the idea of maintaining anonymity online is becoming increasingly unlikely. The issue of anonymity from state surveillance is particularly relevant today given the use of facial recognition software [Woollacott 2021], this software often has access to social media. For instance, Clearview AI provides police agencies and states with facial recognition software that uses a database of 20 billion facial images from a variety of sources including social media [Clearview AI]. Under these conceptions of privacy, as wearable devices featuring cameras become more and more prevalent, it is becoming harder to maintain a sense of privacy. Iachello and Hong also refer to a fundamental article often cited in privacy literature called The Right to Privacy, by Judges Warren and Brandeis, these judges argued: “individuals should be protected from unwarranted publications of any details of their personal life that they might want to keep confidential” [Warren and Brandeis 1890]. This is what is now understood to be “the modern concept of informational self-determination” [Iachello and Hong 2017]; if someone wearing a device inadvertently captures a bystander giving away some private information and then distributes that online, then that bystander’s privacy would have been compromised.

<h3> The problem of bystander privacy concerns and wearable technology </h3>

In their research into bystander privacy concerns from internet-connected devices Perez et al. state that privacy concerns arise when a device that “collects sensor data (such as photos, sound or video) can be used to identify third parties when they have not given consent” [Perez et al. 2017]. They list a number of potential types of privacy concerns: facial recognition (through facial recognition software a bystander may be identifiable in a place or situation where they don’t want to be recognized), speech disclosure (capturing what a bystander is saying when they would prefer it to remain private), and location disclosure (sharing the location of a bystander, for instance capturing them on video in a recognizable setting). Of course, these privacy concerns and several others that they list would have been possible with more traditional cameras, but even in the realms of traditional cameras, one survey found that 95% of people care about bystander privacy when photographed in public spaces [Aditya et al. 2016]. Datta et al argue that wearable devices recording bystanders without consent have the potential to cause harm through reputation damage when their images are posted online, and furthermore sharing of this content can reveal location data and other sensitive information [Datta et al 2018]. Unlike the traditional camera, with wearable devices like smart glasses, the camera never gets put down.  Not having to pick the camera up leads to a heightened risk to bystander privacy.
The problem with a camera that is worn and never gets put down is that it is very hard for a bystander to tell whether their privacy is being compromised. Ahmad et al in their research on bystander privacy and Internet of Things (IoT) devices explain the problem with reference to Irwin Altman’s theory on boundary regulation, Altman claimed that being able to assess your own privacy is fundamental to being able to manage your privacy [Ahmad et al 2020]. Ahmad et al focus on IoT devices and give examples such as Amazon Echo, Echo Look, and Nest Cam (all of which are devices with microphones or microphones and cameras) and argue that the prevalence of these devices in and around the home presents serious privacy concerns, as conversations and encounters that once would have been private are now archived in the cloud, and could even be used maliciously without the knowledge of those captured [Ahmad et al 2020]. These same concerns could be seen as even more severe when considering wearable technologies that feature cameras and microphones, as recordings are often also posted to social media. In their research, they carry out an interview study on how people feel about being near IoT devices and they found that whereas with traditional cameras and other recording devices it is clear when you are being recorded, in the realm of IoT devices people experience greater privacy concerns as they can’t tell if they are being recorded. People want a more tangible sense of when they are being recorded [Ahmad et al].

<h3> Solutions to the bystander privacy concern </h3>

Some propose gaining consent from bystanders as a solution to the concern of bystander privacy. One piece of research involving an in-situ exploratory study of bystanders’ perspectives on the use of smart glasses and other devices recording in public uncovered, through interviews, the overwhelming desire of the participants for smart glasses users to gain permission before recording in public [Singhal et al 2016]. However, the researchers note that this is an impractical solution given the number of bystanders that could be recorded in a public setting. Instead, they propose design considerations to make bystanders aware that a device is recording by providing noticeable visual feedback. They also suggest the possibility of automatically removing bystanders from video recordings [Singhal et al 2016].
Many of the solutions proposed to combat bystander privacy concerns in the realm of recording wearable technology are concerned with the software that runs the device. For example, the device could be programmed to recognize tags worn on clothing or gestures (for example a certain type of wave of the arms) and would either stop the recording entirely or remove the recording that features those bystanders [Datta et al 2018]. It has been noted that these solutions using tags or gestures may be theoretically possible but would be hard to implement [Datta et al]. For example, it would require every individual to know the gesture that would trigger the device to stop recording, or the tags that are to be worn on clothing if you don’t want to be recorded would have to be widely available. Furthermore, in the case of gestures, this solution ignores the criticism of wearable devices that bystanders are often not able to discern whether these devices are recording.
Other software-based solutions propose making use of machine learning algorithms to remove sensitive content captured by wearable devices. Zarapour et al used deep convolutional neural networks to protect sensitive subjects like people, objects (e.g. computer or mobile screens, bank cards), and locations like bathrooms and bedrooms. Using these techniques images are either blurred or removed. Using these techniques, researchers produced 70% accuracy [Zarepour et al. 2016]. These deep learning techniques could prove to be very successful in the future, particularly with removing content that is clearly sensitive like trips to a public bathroom, however, they do little to deal with bystander concerns in less clearcut situations (how can the device tell that a conversation is private when another is not), and they also do not deal with the criticism that many devices do not provide significant visual cues when they are recording.
Perez et al propose a wearable system called the Facial Privacy Enabled Technology (FacePET) system as a solution to the problem of bystander privacy, this system provides a way for a wearer to protect their own privacy rather than relying on the system in the device that is recording them [Perez et al. 2020]. The FacePET system is designed to confuse facial recognition algorithms using LEDs so that if the wearer’s image is captured whilst the FacePET system is turned on, a facial recognition algorithm seeing the picture will not be able to recognise the wearer’s face, thus protecting their privacy from unauthorized face detection and preventing identification [Perez et al 2020]. This solution would help to preserve anonymity, one of the fundamental concepts of privacy. Through a user study of 21 participants, Perez et al found that 16 participants would use a device like the FacePET system and 17 believed smart glasses would become more popular if they had features that could be used to conceal the wearer’s identity [Perez et al. 2020].
In response to the problem that bystanders struggle to distinguish between whether or not a device is recording, Ahmad et al argue that all IoT devices (including wearable devices) should be designed so that they unambiguously demonstrate the state of recording to bystanders to make them aware when their privacy is being compromised. They introduce the idea of “tangible privacy” which they see as essential for boundary regulation [Ahmad et al 2020]. Having conducted in-depth interview research they provide several solutions which would provide this tangible privacy. They are in favour of solutions that assure bystanders of their privacy, for instance, whilst they believe that LEDs are necessary to demonstrate that a device is recording, alone they are insufficient [Ahmad et al 2020]. LEDs provide visual feedback that a device’s sensors are recording, however, these researchers found that the absence of an LED was insufficient to instil trust in the user or bystander that the device was not recording. They found bystanders typically do not know how to interpret the LED and participants exhibited confusion [Ahmad et al 2020]. Furthermore, they argue that the LED may even not be noticeable to a bystander who therefore would not know that the device was being recorded, this conclusion is in line with the criticism by the DPC of the LED being too small on the Ray-Ban Stories glasses [Lomas 2022]. Instead, Ahmad et al argue that their findings demonstrate a need for a device to have a physical shutter mechanism for a camera and a hardware switch to disable its microphone. Ahmad et al found that in certain situations participants preferred to unplug a device rather than use its mute function, they argue that rather than relying on software solutions to shut off a sensor that users will not always trust, if these sensors can be manually shut off then users and bystanders would have a tangible sense of their privacy [Ahmad 2020].

<h2> Proposed solution </h2>

<h3> Background for the proposed solution </h3>

The FacePET solution that distorts the wearer’s face to facial recognition algorithms [Perez et al. 2020] offers a promising solution to the issue of bystander privacy by providing a technological means to conceal the wearer’s identity. The FacePET prototype was bulky and unsightly, but the research found that participants believed smart glasses would become more popular if they provided this same solution [Perez et al. 2020]. However, whilst promising, this solution is not alone sufficient to solve the problem of bystander privacy as it presumes that bystanders should have to opt out to maintain their privacy.  
Jeff Martin, VP of product at Finite State argues that smart glasses, as devices that capture bystanders, are different to other wearable technologies like smartwatches where only the user’s privacy and security are threatened. As a wearer of a smartwatch, the user has made an informed decision or opt-in and the privacy and security risks fall on the owner [Mikalauskas 2021]. Devices with outward-facing cameras, however, capture images of non-users who have not made the informed decision to opt-in. In general privacy and security regulations work on the principle that when giving away sensitive or private data individuals are required to opt in, not to opt out if they wish to maintain their privacy, and regulation on wearable devices with cameras should follow the same trend [Mikalauskas 2021]. Therefore, whilst a promising technological solution to conceal a bystander’s identity, smart glasses featuring FacePET technology or similar are not a sufficient solution alone to the problem as they are based on an opt-out rather than opt-in. Such a solution would run into issues of affordability and availability (only those who can afford and obtain the technology are able to maintain their privacy). Nonetheless, just as an LED demonstrating that a device is recording may not be sufficient alone to solve the problem, it adds an extra layer of protection and the same is certainly true of the FacePET technology.
Ahmad et al propose several facets to their solution of wearable devices providing a sense of tangible privacy to bystanders. They found that bystanders often are unable to tell whether their privacy is being compromised by a device, and so argue that a device should unambiguously demonstrate the state of recording to bystanders. This would enable those around a device to be able to adapt their behaviour in the same way they would if a traditional camera were pointed at them. They propose the use of solutions that assure bystanders of their privacy such as camera lens covers and physical mute buttons. These principles are fundamental to my solution moving forward.

<h3> Design of the solution </h3>

I propose developing the ideas of these researchers to provide a solution that offers bystanders a tangible sense of privacy, as well as protecting the wearer of the device when they become the bystander. I propose that a wearable device such as a pair of smart glasses should make use of physical characteristics to demonstrate to bystanders when their privacy is compromised, in line with Ahmad et al.’s position on demonstrating the recording state unambiguously.
In proposing this solution I will focus on a pair of smart glasses, though these principles could be applied to any wearable device that features a camera and microphone.
This device should have camera lens shutters, a clearly visible mute/recording sliding button, and a clearly noticeable LED (larger than the one featured on the Ray-Ban Stories glasses) that is illuminated when the device is recording. The LED would sit by one or both camera lenses, and the mute/recording button would sit on the glasses’ arm (also called a temple). The camera lens shutters and the mute/recording button should, by default, be closed and set to “mute”. They should be capable of closing electronically, but only sliding the mute/recording button manually would unmute the microphone and open the camera shutters, providing an unambiguous display that recording is starting. The lens shutters would match the design of glasses to avoid being unsightly (which would prevent adoption), but the device should be designed in such a way that when recording the lenses, LED, and recording sliding button are all clearly visible.
The device should also offer the wearer a sense of privacy by providing the functionality of the FacePET system [Perez et al. 2020] which, if the wearer becomes a bystander, could be turned on by the user thus preventing any images captured of their face from being recognizable by facial recognition algorithms.  This solution to privacy in wearable technology can be seen as a privacy-first design, one that clearly demonstrates a recording or not recording state to bystanders, offering a tangible sense of privacy and unambiguously demonstrating when privacy is compromised, as well as being capable of protecting the identity of the wearer themselves when they become bystanders.
I will not be working on the functionality of the FacePET system. For more information on the FacePET system please see the research provided by Perez et al. [Perez et al. 2020].

<h3> Design Principles to follow </h3>

The users of smart glasses may be of any age, gender, ethnicity, physical and cognitive ability, education, or social background. Some smart glasses may be used in work environments, though they will also be used for pleasure. Given the very wide range of possible users it is important to ensure that the features proposed are accessible to all, and so must be easy to use and intuitive.
My solution does not propose a change to the User Interface of any specific pair of smart glasses, though it does propose changes to the physical characteristics by including the addition of camera shutters and a microphone mute/recording button. This solution is designed to be very easy to use, satisfying the important principle of usability. Operating the mute/recording button will open the camera lenses and enable the microphone, enabling the user to record content. Operating it again will immediately stop recording, closing the shutters and muting the microphone, alternatively shutting it off could be controlled by the user electronically. Activating the FacePET technology [Perez et al. 2020] will be a simple on-off switch; in the case of glasses without a user display (like the Ray-Ban Stories) this would be a button toggle, in other glasses with a UI it could be controlled with a touch button interacting with the user’s display in the lenses. The means of activating the FacePET technology must be easily accessible to ensure usability, but it is not important how it is activated. The mute/recording switch however must be physical, positioned so that it is clearly visible to a bystander.
This solution to bystander privacy concerns is simple by design, it should be easy to understand the recording state the device is in for any user or bystander. As a result, this simplicity not only caters to universal usability but also ensures providing the user with an internal locus of control (changing the recording state is very easy), there is no burden on short term memory load, and it would be difficult to make an error. It, therefore, satisfies several important design principles, originally proposed by Schneiderman for UI design [Wong 2018]. This system may not have a UI but applying the same principles to the use of devices without a UI ensures a positive user experience.  

<h2>  Proposed study design </h2>

<h3> Introduction </h3>

The intention of my research is to ascertain whether the solution that I have proposed, smart glasses including camera shutters, LED to display recording, mute/recording button and FacePET technology gives bystanders and users alike a greater sense of privacy when compared to traditional devices.  Through my research, I will seek to answer the research question: How does the type of smart glasses worn by a user influence a bystander’s sense of privacy? I will do this through a field experiment and structured interviews.

<h3> Study design </h3>

In order to conduct this research, I will use the field experiment technique, manipulating the independent variable of the type of smart glasses worn by a researcher. The experiment will take place on Swansea University Campus, with the permission of the university. In total there will be 6 sessions, taking place in 3 different locations across the university (2 sessions for each location). The locations are in the sports centre, in the computational foundry, and outside the engineering building. The various locations are chosen to account for location-dependent varying desires for privacy (participants may prefer greater privacy inside vs outside or greater privacy in the sports centre, for example). The sessions will take place on different days of the week and at different times of day, to hopefully have a greater variety of participants.

<h3> The sessions </h3>

Session1:
A researcher will stand in the location for one hour using a pair of Ray-Ban Stories smart glasses to take pictures and record videos. The researcher will stand alternating between looking around without taking photos for two minutes, followed by taking photos and videos for the next two minutes. Exposing bystanders to both recording and not recording states. A second researcher will approach individuals who have passed by to ask if they would happily take part in a short interview (interview 1).
Session2:
This session works in the same way, but the researcher will wear a prototype pair of smart glasses, designed to look like Ray-Ban Stories but featuring camera shutters, and a mute/recording button, and will feature FacePET technology.
After each session, bystanders willing to participate will take part in interview 1, then be briefed about the purpose of the FacePET system and will be asked by the interviewer to walk past the field researcher whilst wearing the prototype glasses with the FacePET system enabled. They will then be asked a further series of questions (interview 2).

<h3> The interviews </h3>

The interviews will be structured interviews that should last no longer than five minutes. Interview 1 will include the following questions:
-         Did you notice that the individual (the researcher) was recording or taking photos?
-         Could you tell when the individual started recording? How could you tell?
-         Could you tell when the individual was not recording? How could you tell?
-         When the individual was not recording, did you feel more or less comfortable?
-         Did you feel the individual’s recording threatened your privacy?
-         When the individual was not recording did you have concerns for your privacy?
 
Interview 2 will take place after the participant has used the prototype smart glasses with the FacePET system enabled, and will include the following questions:
-         Does the FacePET system make you feel more or less comfortable when being recorded by a stranger?
-         Do you feel more or less concerned for your privacy when wearing the FacePET enabled glasses vs not wearing them?
-         Would you wear glasses that feature the FacePET system?
-         Do you trust the FacePET system?
 
Responses to these interviews will be audio-recorded for later analysis by researchers.

<h3> Hypotheses </h3>

My first hypothesis: participants will find it easier to distinguish recording states in the case of the prototype glasses when compared to the Ray-Ban Stories.
My second hypothesis: being able to tell when the glasses are and are not recording will make participants feel more comfortable and less concerned about their privacy.
My third hypothesis: participants will feel less concerned for their privacy when wearing the FacePET system when being recorded by the researcher.

<h3> Measurements </h3>

The study will collect responses to interview questions as measurements, the majority of interview questions are closed-ended to enable easier analysis of data. If participants can distinguish between recording states with the prototype vs Ray-Ban Stories this will support hypothesis 1. If fewer participants are concerned for their privacy when the prototype device is not recording vs when the ray-ban Stories are not recording, this will support hypothesis 2. If participants feel more comfortable and less concerned for privacy when wearing FacePET enabled glasses vs not wearing FacePET enabled glasses whilst being recorded, hypothesis 3 will be supported. How significantly these findings support these hypotheses will be determined by the distribution of responses in terms of percentages of participants answering one way vs another.

<h3> Limitations and evaluation </h3>

In order to test the impact of this solution, I have proposed a field experiment which can provide real-world responses from participants. This type of study, however, suffers some limitations due to the lack of control of extraneous variables, for instance, bystanders may not wish to participate, or there may be too few participants available, and the experiment will be difficult to replicate which may bring into question validity. This study could be criticised on ethical grounds due to deliberately threatening the privacy of participants, however, the images and video captured will not be distributed online and the smart glasses used will not have more pervasive abilities than other devices that are already widely used in public today. One reasonable concern is that through chance alone participants in one session may have far greater concern for privacy than in another.  However, despite the limitations of a field experiment, as participants are not made aware of the researcher prior to taking part, their real-world responses provide a higher level of ecological validity, justifying the use of this type of study.
For this study to be effective, the prototype of my solution would have to appear like a traditional pair of smart glasses so as not to draw undue attention. This would require significant work and so could be a limitation to this study. To combat this limitation, I will focus on developing a prototype which at least features the LED, camera shutters, and clearly visible mute/recording button. This would be essential to testing whether these changes impact bystander privacy concerns. Ideally, glasses that appear mostly normal featuring working FacePET technology would be ideal. However, as the FacePET technology works using LEDs and is at an early stage, a working prototype is likely to be bulky and unsightly. If the system cannot be concealed in a normal-looking pair of glasses, it may be beneficial to the experiment to gauge participant responses to the system without a working FacePET system. That way, participants could be told “these glasses prevent facial recognition” and their responses could be gauged through interview 2, whilst not requiring them to wear a pair of glasses which are extremely noticeable. If this were to be the case, to avoid ethical implications it would be essential to debrief participants and explain to them that these glasses do not actually contain a working FacePET system, though working prototypes are being developed.


<h2> Conclusion </h2>

Two fundamental concepts of privacy are anonymity and informational self-determination [Iachello and Hong 2017].  It is these two concepts of privacy that wearable devices featuring cameras and microphones predominantly threaten. Today, anonymity is threatened due to the rise of facial recognition software. Facial recognition surveillance is being rolled out fast, the UK police have been accused by over 30 civil liberties groups of bypassing Parliament and introducing live facial recognition technology [Woollacott 2021]. This technology is powered by private companies like Clearview AI, whose intelligence platform includes the world’s largest database of over 20 billion facial images sourced from public platforms including social media [Clearview AI]. Given that today your face is potentially enough to identify you, each time your image is posted online without your consent your privacy is fundamentally jeopardized. Wearable devices pose a significant threat here due to their pervasive and omnipresent nature, unlike traditional cameras, they are not put down. If, as a bystander, aspects of your personal life are captured without your consent and then distributed online, it is not just your anonymity that is threatened but your right to informational self-determination too.
In this paper, I have examined several pieces of relevant literature, covering the academic background of privacy, relevant studies in HCI that examine the problem of privacy concerns for bystanders of wearable technology, and studies that propose solutions for wearable technology. Influenced by the concerns for privacy rights posed by smart glasses, the solution I have proposed builds on the work of Ahmad et al. and Perez et al.. I suggest that smart glasses and other wearable technology must provide a tangible sense of privacy for bystanders by using camera shutters, a physical and clearly visible mute/recording button, and an LED that illuminates when recording. Ensuring that bystanders are unambiguously aware that a recording is taking place will enable them to tailor their actions to the situation in the way they would if a traditional camera were pointed at them (for instance they could stop what they’re doing or even turn around and avoid the camera). This solution also protects the wearer as a bystander through FacePET technology that prevents facial recognition. This device can therefore be seen as a privacy-first solution, protecting both the user’s and bystander’s privacy. This device may be used by any individual in any scenario, and so is designed with universal usability in mind. In order to test the impact of this solution, I propose a study that uses a field experiment and structured interviews with participants to gauge the responses concerning privacy issues caused by a traditional pair of smart glasses (Ray-Ban Stories) vs a prototype of the smart glasses put forward in my solution.
